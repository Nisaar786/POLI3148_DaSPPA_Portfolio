---
title: "Tokenization"
author: "Nisaar Hussain"
output: pdf_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## Introduction

Tokenization, basic text wrangling, and exploratory data analysis.

## Load Library
```{r}
library(tidyverse)
library(lubridate)
```

```{r}
d_fulltext = read_rds("Lecture 10/1_Text_Mining/data/data_ce_speech_article.rds")
# Change the date variable to "date" format
d_fulltext = d_fulltext |> mutate(date_of_speech = dmy(date_of_speech))
```

## Tokenization

```{r}
# Check if tidytext package exists, if not then install it 
if (!require("tidytext")) install.packages("tidytext")

library(tidytext) # Full introduction: http://tidytextmining.com/
```

```{r}

d_fulltext <- d_fulltext |>
  mutate(text = str_replace_all(text, "Hong Kong", "HongKong"))

# Tokenization is splitting the words into small unit of analysis
# We care about the key words
# Have a unique identifier for each document
d_tokenized = d_fulltext |>
  select(uid, date_of_speech, text) |>
  unnest_tokens(word, text) # We convert the text into words spread them by rows 
# first arg: output name; second: input name

head(d_tokenized, 20)

# Simple?
```

## Wrangling: Remove Stop Words

```{r}
# Load Stopwords
# Stopwords are words that do not contain informative meanings 
# e.g. conjuctions
data("stop_words") # stopwords dataset provided in the R system 

head(stop_words, 20)
```

```{r}
# Remove stopwords
# Use anti-join
d_tokenized_s = d_tokenized |>
  anti_join(stop_words, by = "word")
# anti_join: whatever appearing in the stop_words dataframe, we remove it.

# We notice a sharp decrease in the dataset after removing stop words 
```

## Wrangling [Optional]: Stemming

```{r}
if (!require(SnowballC)) install.packages("SnowballC")
library(SnowballC)

# Stemming - comfortable is reduced to comfort, ran, run - convert to run
# linguistic transformation to harmonize the words that are same   
```

```{r}
d_tokenized_s = d_tokenized_s |>
  mutate(stem = wordStem(word))

head(d_tokenized_s, 20)
```

## Exploratory Data Analysis

### Count word frequencies

```{r}
# Count term frequencies (for raw words)
# Count most frequent words
word_frequency = d_tokenized_s |>
  count(word, sort = TRUE)

head(word_frequency, 20)

# Count term frequencies (for Stemmed word -- recommended)
word_frequency = d_tokenized_s |>
  count(stem, sort = TRUE) |>
  rename("word" = "stem")

head(word_frequency, 20)
```

### Examine most popular words

```{r}
# Get a subset of most frequent words
word_frequency_top = word_frequency |>
  arrange(desc(n)) |> # Make sure that it is sorted properly
  slice(1:200) # Take the first 200 rows. 
```

### Plot most popular words

```{r}
# Plot top 10 most popular words
word_frequency_top |>
  slice(1:10) |>
  mutate(word = reorder(word, n)) |>
  ggplot(aes(x = n, y = word)) +
  geom_col() +
  theme_bw()
```

### Plot a Word Cloud

```{r}
# A word cloud describing the most popular words 
if (!require(ggwordcloud)) install.packages("ggwordcloud")
library(ggwordcloud)

word_frequency_top |>
  slice(1:100) |>
  ggplot(aes(label = word, size = n)) +
  scale_size_area(max_size = 10) +
  geom_text_wordcloud() +
  theme_minimal()

# If a lot of words are clustered in the middle of the figure
# You have to reduce the max_size

```

```{r}
# An alternative wordcloud package
if (!require(wordcloud)) install.packages("wordcloud")
library(wordcloud)

# use the $ to refer to the required columns
wordcloud(
  word_frequency_top$word, word_frequency_top$n, 
  rot.per = 0, random.order = FALSE, random.color = TRUE)

```

```{r, results='hide'}
# The third wordcloud package
# https://r-graph-gallery.com/196-the-wordcloud2-library.html
if (!require(wordcloud2)) install.packages("wordcloud2")
library(wordcloud2)

wordcloud2(word_frequency_top)

wordcloud2(word_frequency_top, shape = "star")

wordcloud2(word_frequency_top, shape = "pentagon")

# It makes the word cloud more colorful and customized
# It requires a clean dataset with few columns for recognition 


```

## Comparative Exploratory Analysis

How does the focus differ between 2021 and 2020?

```{r}
# We want to compare top word frequencies for 2 documents 
# E.g. Calculate term frequencies for 2020 and 2021 respectively

word_frequency_compare_21_20 = 
  d_tokenized_s |>
  mutate(year = year(date_of_speech), .after = "date_of_speech") |>
  # Extract the year of the speech
  filter(year == 2020 | year == 2021) |>
  group_by(year, stem) |>
  count(sort = TRUE) |>
  pivot_wider(names_from = "year", values_from = "n", 
              names_prefix = "n_", values_fill = 0) |>
  ungroup() |>
  mutate(
    prop_2021 = n_2021 / sum(n_2021), # Calculating the proportions
    prop_2020 = n_2020 / sum(n_2020)
  )
    
```

```{r}
# Visualize the word frequencies in the two years
word_frequency_compare_21_20 |>
  ggplot(aes(x = prop_2020, y = prop_2021)) +
  geom_point()

word_frequency_compare_21_20 |>
  ggplot(aes(x = prop_2020, y = prop_2021)) +
  geom_point() +
  scale_x_sqrt() + scale_y_sqrt()


word_frequency_compare_21_20 |>
  ggplot(aes(x = log(prop_2020), y = log(prop_2021))) +
  geom_point()

word_frequency_compare_21_20 |>
  filter(n_2020 >= 10) |>
  ggplot(aes(x = log(prop_2020), y = log(prop_2021))) +
  geom_point() +
  geom_smooth()

# The frequency of the words in 2 years are relatively similar from the final figure

```

```{r}
# The biggest difference in the speeches

## What are the words that feature 2020 speeches
# Subtract the proportion of the top 30 words between the two years
tmp_plot_20 = word_frequency_compare_21_20 |>
  mutate(diff = prop_2020 - prop_2021) |>
  slice_max(diff, n = 30) |>
  arrange(desc(diff))
  
## What are the words that feature 2021 speeches
tmp_plot_21 = word_frequency_compare_21_20 |>
  mutate(diff = prop_2021 - prop_2020) |>
  slice_max(diff, n = 30) |>
  arrange(desc(diff))

```

```{r}
# Visualize the difference in a nice way?
# Create a comparitive wordcloud for 2021 and 2020

# install.packages("ggwordcloud")

library(ggwordcloud)


set.seed(327)
tmp_plot_merge = tmp_plot_21 |> 
  mutate(Year = "2021") |>
  bind_rows(
    tmp_plot_20 |> mutate(Year = "2020")
    ) 

tmp_plot_merge |>
  ggplot(aes(label = stem, x = Year, color = Year, size = abs(diff))) +
  geom_text_wordcloud_area() +
  scale_size_area(max_size = 14) +
  theme_minimal() +
  theme(legend.position = "top")

tmp_plot_merge |>
  ggplot(aes(label = stem, y = Year, color = Year, size = abs(diff))) +
  geom_text_wordcloud_area() +
  scale_size_area(max_size = 14) +
  theme_minimal() +
  theme(legend.position = "top")


```
